
@article{kim_chatgpt_2023,
	title = {{ChatGPT} and large language model ({LLM}) chatbots: {The} current state of acceptability and a proposal for guidelines on utilization in academic medicine},
	volume = {19},
	issn = {14775131},
	shorttitle = {{ChatGPT} and large language model ({LLM}) chatbots},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1477513123002243},
	doi = {10.1016/j.jpurol.2023.05.018},
	language = {en},
	number = {5},
	urldate = {2024-05-02},
	journal = {Journal of Pediatric Urology},
	author = {Kim, Jin K. and Chua, Michael and Rickard, Mandy and Lorenzo, Armando},
	month = oct,
	year = {2023},
	pages = {598--604},
}

@article{thirunavukarasu_large_2023,
	title = {Large language models in medicine},
	volume = {29},
	issn = {1078-8956, 1546-170X},
	url = {https://www.nature.com/articles/s41591-023-02448-8},
	doi = {10.1038/s41591-023-02448-8},
	language = {en},
	number = {8},
	urldate = {2024-05-02},
	journal = {Nature Medicine},
	author = {Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei},
	month = aug,
	year = {2023},
	pages = {1930--1940},
}

@misc{jesse_sumrak_7_2024,
	title = {7 {LLM} use cases and applications in 2024},
	url = {https://www.assemblyai.com/blog/llm-use-cases/},
	abstract = {Learn about the top LLM use cases and applications in 2024 for streamlining business operations, automating mundane tasks, and tackling challenges.},
	language = {en},
	urldate = {2024-05-03},
	journal = {News, Tutorials, AI Research},
	author = {{Jesse Sumrak}},
	month = mar,
	year = {2024},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\87KBVY7V\\llm-use-cases.html:text/html},
}


@misc{noauthor_what_nodate,
	title = {What is an {AI} {Prompt}? {\textbar} {Definition} from {TechTarget}},
	shorttitle = {What is an {AI} {Prompt}?},
	url = {https://www.techtarget.com/searchenterpriseai/definition/AI-prompt},
	abstract = {Gain insights into the importance and functionality of AI prompts. Delve into their pros and cons and discover the art of crafting efficient prompts.},
	language = {en},
	urldate = {2024-05-03},
	journal = {Enterprise AI},
	file = {Snapshot:files/8/AI-prompt.html:text/html},
}

@article{heston_prompt_2023,
	title = {Prompt {Engineering} in {Medical} {Education}},
	volume = {2},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2813-141X},
	url = {https://www.mdpi.com/2813-141X/2/3/19},
	doi = {10.3390/ime2030019},
	abstract = {Artificial intelligence-powered generative language models (GLMs), such as ChatGPT, Perplexity AI, and Google Bard, have the potential to provide personalized learning, unlimited practice opportunities, and interactive engagement 24/7, with immediate feedback. However, to fully utilize GLMs, properly formulated instructions are essential. Prompt engineering is a systematic approach to effectively communicating with GLMs to achieve the desired results. Well-crafted prompts yield good responses from the GLM, while poorly constructed prompts will lead to unsatisfactory responses. Besides the challenges of prompt engineering, significant concerns are associated with using GLMs in medical education, including ensuring accuracy, mitigating bias, maintaining privacy, and avoiding excessive reliance on technology. Future directions involve developing more sophisticated prompt engineering techniques, integrating GLMs with other technologies, creating personalized learning pathways, and researching the effectiveness of GLMs in medical education.},
	language = {en},
	number = {3},
	urldate = {2024-05-03},
	journal = {International Medical Education},
	author = {Heston, Thomas and Khun, Charya},
	month = aug,
	year = {2023},
	pages = {198--205},
	file = {Texto completo:files/10/Heston y Khun - 2023 - Prompt Engineering in Medical Education.pdf:application/pdf},
}

@misc{edi_hezri_hairi_2_2023,
	title = {(2) 9 {Frameworks} to master {ChatGPT} {Prompt} {Engineering}. {\textbar} {LinkedIn}},
	url = {https://www.linkedin.com/pulse/9-frameworks-master-chatgpt-prompt-engineering-edi-hezri-hairi/},
	urldate = {2024-05-04},
	author = {{Edi Hezri Hairi}},
	month = aug,
	year = {2023},
	file = {(2) 9 Frameworks to master ChatGPT Prompt Engineering. | LinkedIn:C\:\\Users\\Usuario\\Zotero\\storage\\THX5B42C\\9-frameworks-master-chatgpt-prompt-engineering-edi-hezri-hairi.html:text/html},
}


@misc{javier_canales_luna_what_2023,
	title = {What is an {LLM}? {A} {Guide} on {Large} {Language} {Models} and {How} {They} {Work}},
	shorttitle = {What is an {LLM}?},
	url = {https://www.datacamp.com/blog/what-is-an-llm-a-guide-on-large-language-models},
	abstract = {Read this article to discover the basics of large language models, the key technology that is powering the current AI revolution},
	language = {en},
	urldate = {2024-05-05},
	author = {{Javier Canales Luna}},
	month = dec,
	year = {2023},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\DLTDATKW\\what-is-an-llm-a-guide-on-large-language-models.html:text/html},
}


@misc{noauthor_106_nodate,
	title = {10.6. {The} {Encoder}–{Decoder} {Architecture} — {Dive} into {Deep} {Learning} 1.0.3 documentation},
	url = {https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html},
	urldate = {2024-05-05},
	file = {10.6. The Encoder–Decoder Architecture — Dive into Deep Learning 1.0.3 documentation:files/16/encoder-decoder.html:text/html},
}

@misc{kostadinov_understanding_2019,
	title = {Understanding {Encoder}-{Decoder} {Sequence} to {Sequence} {Model}},
	url = {https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346},
	abstract = {In this article, I will try to give a short and concise explanation of the sequence to sequence model which have recently achieved…},
	language = {en},
	urldate = {2024-05-05},
	journal = {Medium},
	author = {Kostadinov, Simeon},
	month = nov,
	year = {2019},
	file = {Snapshot:files/18/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346.html:text/html},
}

@misc{kumar_demystifying_2024,
	title = {Demystifying {Encoder} {Decoder} {Architecture} \& {Neural} {Network}},
	url = {https://vitalflux.com/encoder-decoder-architecture-neural-network/},
	abstract = {Encoder decoder architecture, Encoder Architecture, Decoder Architecture, BERT, GPT, T5, BART, Examples, NLP, Transformers, Machine Learning},
	language = {en-US},
	urldate = {2024-05-05},
	journal = {Analytics Yogi},
	author = {Kumar, Ajitesh},
	month = jan,
	year = {2024},
	file = {Snapshot:files/20/encoder-decoder-architecture-neural-network.html:text/html},
}

@misc{ahmadsabry_perfect_2023,
	title = {A {Perfect} guide to {Understand} {Encoder} {Decoders} in {Depth} with {Visuals}},
	url = {https://medium.com/@ahmadsabry678/a-perfect-guide-to-understand-encoder-decoders-in-depth-with-visuals-30805c23659b},
	abstract = {Introduction},
	language = {en},
	urldate = {2024-05-05},
	journal = {Medium},
	author = {Ahmadsabry},
	month = jun,
	year = {2023},
	file = {Snapshot:files/22/a-perfect-guide-to-understand-encoder-decoders-in-depth-with-visuals-30805c23659b.html:text/html},
}

@article{peng_instruction_2023,
	title = {Instruction {Tuning} with {GPT}-4},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2304.03277},
	doi = {10.48550/ARXIV.2304.03277},
	abstract = {Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.},
	urldate = {2024-05-05},
	author = {Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
	year = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
	annote = {Other
8 pages. Work in progress. Project page: https://instruction-tuning-with-gpt-4.github.io},
}

@misc{noauthor_rag_nodate,
	title = {{RAG} - {Retrieval} {Augmented} {Generation}: {The} key that unlocks the door to precision language models {\textbar} datos.gob.es},
	url = {https://datos.gob.es/en/blog/rag-retrieval-augmented-generation-key-unlocks-door-precision-language-models},
	urldate = {2024-05-07},
	file = {RAG - Retrieval Augmented Generation\: The key that unlocks the door to precision language models | datos.gob.es:files/26/rag-retrieval-augmented-generation-key-unlocks-door-precision-language-models.html:text/html},
}

@misc{noauthor_what_nodate-2,
	title = {What is {Retrieval} {Augmented} {Generation} ({RAG})? {A} {Guide} to the {Basics}},
	shorttitle = {What is {Retrieval} {Augmented} {Generation} ({RAG})?},
	url = {https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag},
	abstract = {Explore Retrieval Augmented Generation (RAG) RAG: Integrating LLMs with data search for nuanced AI responses. Understand its applications and impact.},
	language = {en},
	urldate = {2024-05-07},
	file = {Snapshot:files/28/what-is-retrieval-augmented-generation-rag.html:text/html},
}

@misc{pubmed_1996,
	title = {About},
	howpublished ={https://pubmed.ncbi.nlm.nih.gov/about/},
    year = {1996},
	abstract = {PubMed about page.},
	language = {en},
	urldate = {2024-05-07},
	journal = {PubMed},
	file = {Snapshot:files/30/about.html:text/html},
}

@article{habas_resolution_2020,
	title = {Resolution of coronavirus disease 2019 ({COVID}-19)},
	volume = {18},
	issn = {1478-7210},
	url = {https://doi.org/10.1080/14787210.2020.1797487},
	doi = {10.1080/14787210.2020.1797487},
	abstract = {Coronavirus disease 2019 (COVID-19) was first detected in China in December, 2019, and declared as a pandemic by the World Health Organization (WHO) on March 11, 2020. The current management of COVID-19 is based generally on supportive therapy and treatment to prevent respiratory failure. The effective option of antiviral therapy and vaccination are currently under evaluation and development. A literature search was performed using PubMed between December 1, 2019–June 23, 2020. This review highlights the current state of knowledge on the viral replication and pathogenicity, diagnostic and therapeutic strategies, and management of COVID-19. This review will be of interest to scientists and clinicians and make a significant contribution toward development of vaccines and targeted therapies to contain the pandemic. The exit strategy for a path back to normal life is required, which should involve a multi-prong effort toward development of new treatment and a successful vaccine to protect public health worldwide and prevent future COVID-19 outbreaks. Therefore, the bench to bedside translational research as well as reverse translational works focusing bedside to bench is very important and would provide the foundation for the development of targeted drugs and vaccines for COVID-19 infections.},
	number = {12},
	urldate = {2024-05-07},
	journal = {Expert Review of Anti-infective Therapy},
	author = {Habas, Khaled and Nganwuchu, Chioma and Shahzad, Fanila and Gopalan, Rajendran and Haque, Mainul and Rahman, Sayeeda and Majumder, Anwarul Azim and Nasim, Talat},
	month = dec,
	year = {2020},
	pmid = {32749914},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/14787210.2020.1797487},
	keywords = {COVID-19, diagnosis, SARS-CoV-2, severe acute respiratory syndrome coronavirus-2, treatment},
	pages = {1201--1211},
}

@article{clusmann_future_2023,
	title = {The future landscape of large language models in medicine},
	volume = {3},
	copyright = {2023 The Author(s)},
	issn = {2730-664X},
	url = {https://www.nature.com/articles/s43856-023-00370-1},
	doi = {10.1038/s43856-023-00370-1},
	abstract = {Large language models (LLMs) are artificial intelligence (AI) tools specifically trained to process and generate text. LLMs attracted substantial public attention after OpenAI’s ChatGPT was made publicly available in November 2022. LLMs can often answer questions, summarize, paraphrase and translate text on a level that is nearly indistinguishable from human capabilities. The possibility to actively interact with models like ChatGPT makes LLMs attractive tools in various fields, including medicine. While these models have the potential to democratize medical knowledge and facilitate access to healthcare, they could equally distribute misinformation and exacerbate scientific misconduct due to a lack of accountability and transparency. In this article, we provide a systematic and comprehensive overview of the potentials and limitations of LLMs in clinical practice, medical research and medical education.},
	language = {en},
	number = {1},
	urldate = {2024-05-08},
	journal = {Communications Medicine},
	author = {Clusmann, Jan and Kolbinger, Fiona R. and Muti, Hannah Sophie and Carrero, Zunamys I. and Eckardt, Jan-Niklas and Laleh, Narmin Ghaffari and Löffler, Chiara Maria Lavinia and Schwarzkopf, Sophie-Caroline and Unger, Michaela and Veldhuizen, Gregory P. and Wagner, Sophia J. and Kather, Jakob Nikolas},
	month = oct,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Health services, Medical research, Public health},
	pages = {1--8},
	file = {Full Text PDF:files/33/Clusmann et al. - 2023 - The future landscape of large language models in m.pdf:application/pdf},
}

@misc{nucci_large_2023,
	title = {Large {Language} {Models} in {Healthcare}: {Use} {Cases} and {Benefits}},
	shorttitle = {Large {Language} {Models} in {Healthcare}},
	url = {https://aisera.com/blog/large-language-models-healthcare/},
	abstract = {Large language models revolutionize healthcare with innovative diagnostics, clinical documentation, and patient care solutions. Learn more!},
	language = {en-US},
	urldate = {2024-05-08},
	journal = {Aisera: Best Generative AI Platform For Enterprise},
	author = {Nucci, Antonio},
	month = dec,
	year = {2023},
	file = {Snapshot:files/35/large-language-models-healthcare.html:text/html},
}

@misc{malge_building_2024,
	title = {Building {Q}-{A} {Chat} {Bot}: {Gemma}({LLM}) with {Hugging} {Face}},
	shorttitle = {Building {Q}-{A} {Chat} {Bot}},
	url = {https://medium.com/@prashantmalge181/building-q-a-chat-bot-gemma-llm-with-hugging-face-233ddcc76fe8},
	abstract = {Introduction},
	language = {en},
	urldate = {2024-05-08},
	journal = {Medium},
	author = {Malge, Prashant},
	month = mar,
	year = {2024},
	file = {Snapshot:files/37/building-q-a-chat-bot-gemma-llm-with-hugging-face-233ddcc76fe8.html:text/html},
}


@misc{harry_guinness_best_2024,
	title = {The best large language models ({LLMs}) in 2024},
	url = {https://zapier.com/blog/best-llm/#gemma},
	abstract = {There are dozens of major LLMs, and hundreds that are arguably significant for some reason or other. These are 14 of the best LLMs available now.},
	language = {en},
	urldate = {2024-05-08},
	author = {{Harry Guinness}},
	month = may,
	year = {2024},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\WSYIFKRD\\best-llm.html:text/html},
}


@misc{noauthor_meta_nodate,
	title = {Meta {Llama} 3},
	url = {https://llama.meta.com/llama3/},
	abstract = {Build the future of AI with Meta Llama 3. Now available with both 8B and 70B pretrained and instruction-tuned versions to support a wide range of applications.},
	language = {en},
	urldate = {2024-05-08},
	journal = {Meta Llama},
}

@misc{wu_how_2024,
	title = {How faithful are {RAG} models? {Quantifying} the tug-of-war between {RAG} and {LLMs}' internal prior},
	shorttitle = {How faithful are {RAG} models?},
	url = {http://arxiv.org/abs/2404.10198},
	doi = {10.48550/arXiv.2404.10198},
	abstract = {Retrieval augmented generation (RAG) is often used to fix hallucinations and provide up-to-date knowledge for large language models (LLMs). However, in cases when the LLM alone incorrectly answers a question, does providing the correct retrieved content always fix the error? Conversely, in cases where the retrieved content is incorrect, does the LLM know to ignore the wrong information, or does it recapitulate the error? To answer these questions, we systematically analyze the tug-of-war between a LLM's internal knowledge (i.e. its prior) and the retrieved information in settings when they disagree. We test GPT-4 and other LLMs on question-answering abilities across datasets with and without reference documents. As expected, providing the correct retrieved information fixes most model mistakes (94\% accuracy). However, when the reference document is perturbed with increasing levels of wrong values, the LLM is more likely to recite the incorrect, modified information when its internal prior is weaker but is more resistant when its prior is stronger. Similarly, we also find that the more the modified information deviates from the model's prior, the less likely the model is to prefer it. These results highlight an underlying tension between a model's prior knowledge and the information presented in reference documents.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Wu, Kevin and Wu, Eric and Zou, James},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10198 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:files/43/Wu et al. - 2024 - How faithful are RAG models Quantifying the tug-o.pdf:application/pdf;arXiv.org Snapshot:files/46/2404.html:text/html},
}

@misc{noauthor_what_nodate-3,
	title = {What is retrieval-augmented generation ({RAG})? - {IBM} {Research}},
	url = {https://research.ibm.com/blog/retrieval-augmented-generation-RAG},
	urldate = {2024-05-08},
}

@misc{noauthor_what_nodate-4,
	title = {What {Is} {Retrieval}-{Augmented} {Generation} ({RAG})?},
	url = {https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/},
	urldate = {2024-05-08},
	file = {What Is Retrieval-Augmented Generation (RAG)?:files/45/retrieval-augmented-generation-rag.html:text/html},
}

@misc{natassha_selvaraj_what_2024,
	title = {What is {Retrieval} {Augmented} {Generation} ({RAG})? {A} {Guide} to the {Basics}},
	shorttitle = {What is {Retrieval} {Augmented} {Generation} ({RAG})?},
	url = {https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag},
	abstract = {Explore Retrieval Augmented Generation (RAG) RAG: Integrating LLMs with data search for nuanced AI responses. Understand its applications and impact.},
	language = {en},
	urldate = {2024-05-07},
	author = {{Natassha Selvaraj}},
	month = jan,
	year = {2024},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\UT2RLBUI\\what-is-retrieval-augmented-generation-rag.html:text/html},
}


@article{gomez_aplicacion_2020,
	title = {Aplicación de las {Metodologías} Ágiles al proceso de enseñanza-aprendizaje universitario},
	copyright = {Drets d'autor (c) 2020 Revista d'Innovació Docent Universitària},
	issn = {2013-2298},
	url = {https://revistes.ub.edu/index.php/RIDU/article/view/RIDU2020.12.7},
	doi = {10.1344/RIDU2020.12.7},
	abstract = {Aquesta proposta d'innovació docent té com a objectiu el de dinamitzar i apropar al món empresarial l'ensenyament  en l'assignatura de sis crèdits Gestió i organització d'Empreses II, cursada en el quart semestre dels diferents graus que s'ofereixen a la Facultad de Ciencias Económicas de la Universidad San Pablo-CEU, de forma que l'alumne participi de forma més activa del que fins ara venia fent i s'impliqui en la solució dels problemes d'empreses reals plantejats a l'aula. Amb aquesta finalitat s'utilizaran metodologies àgils i innovacions tecnològiques, que permetin aplicar de forma pràctica l'aprenentatge basat en projectes (APB).Per a la seva implementació es proposa la resolució d'un cas de producció per tallers, mitjançant la formació d'equips àgils, utilitzant Scrum i Kanban, fent servir per al darrer, l'aplicació Trello. L'enfoc iteratiu i incremental d'aquesta metodologia permetrà que l'alumne conegui de forma senzilla i pràctica conceptes que creixen en quant a la seva complexitat, segons es va evolucionant en la resolució del mateix.Per a l'assignatura en qüestió, el desenvolupament de la solució es farà en cinc iteracions o sprints, afegint a cada iteració nous resultats d'aprenentatge, que permetin aconseguir les competències marcades en la matèria.},
	language = {es},
	urldate = {2024-05-30},
	journal = {Revista d'Innovació Docent Universitària},
	author = {Gómez, Sonia Martín},
	month = jan,
	year = {2020},
	keywords = {Trello},
	pages = {62--73},
	file = {Full Text PDF:files/50/Gómez - 2020 - Aplicación de las Metodologías Ágiles al proceso d.pdf:application/pdf},
}

@misc{talent_salario_2024,
	title = {Salario para {Programador} {Junior} en {España} - {Salario} {Medio}},
	howpublished ={https://es.talent.com/salary},
	abstract = {Programador Junior en España perciben un salario medio mensual € 1.750. Prueba la herramienta de salarios de Talent.com y descubre cuál es el salario medio de los profesionales de la industria.},
	language = {es-es},
	urldate = {2024-05-31},
	journal = {Talent.com},
    year = {2024},
	file = {Snapshot:files/54/salary.html:text/html},
}

@misc{Billin_2014, 
    title = {¿Cuánto cuesta contratar a un trabajador?},
    howpublished ={https://www.billin.net/calculadora-contratar-trabajador/}, 
    journal={Billin}, 
    publisher={Billin}, 
    year={2014}
} 


@misc{CE_ley_2024,
	title = {Ley de {IA} {\textbar} {Configurar} el futuro digital de {Europa}},
    author = {Comisión Europea},
	url = {https://digital-strategy.ec.europa.eu/es/policies/regulatory-framework-ai},
	language = {es},
	urldate = {2024-05-31},
	month = apr,
	year = {2024},
	file = {Snapshot:files/60/regulatory-framework-ai.html:text/html},
}

@misc{Anandhu_H_abstracts_2023,
	title = {Abstracts of 10,000 {Covid} {Research} {Papers}},
    author = {Anandhu H},
    year = {2023},
	url = {https://www.kaggle.com/datasets/anandhuh/covid-abstracts},
	abstract = {Title, Abstract and URL of Covid Research Papers},
	language = {en},
	urldate = {2024-06-01},
	file = {Snapshot:files/62/covid-abstracts.html:text/html},
}

@article{meo_anatomy_2018,
	title = {Anatomy and physiology of a scientific paper},
	volume = {25},
	issn = {1319-562X},
	url = {https://www.sciencedirect.com/science/article/pii/S1319562X18300135},
	doi = {10.1016/j.sjbs.2018.01.004},
	abstract = {Writing and publishing a scientific paper in academic journals is a highly competitive, time-consuming stepwise process. The road to scientific writing and publication is rarely straightforward. Scientific writing has uniform format, which is perplexing for the novice science writers due to its inflexible anatomy (structure) and physiology (functions). Many obstacles are allied with the scientific writing path which can be minimized by applying some simple guidelines and practices. The scientific papers have an almost similar format but, original articles are divided into distinct sections and each segment contains a specific type of information. The basic anatomy of scientific papers is mainly comprised of the structure of the various components of a scientific paper, including title, abstract, introduction, methods, results, discussion, conclusion, acknowledgments and references. However, the physiology of a scientific paper is difficult to understand. Early career researchers and trainees may be less familiar with the various components of scientific papers. In this study, we applied an observational approach to describe the essential steps to facilitate the readers and writers to understand the basic characteristics, anatomy and physiology of writing the various sections of a scientific paper for an academic science journal.},
	number = {7},
	urldate = {2024-06-01},
	journal = {Saudi Journal of Biological Sciences},
	author = {Meo, Sultan Ayoub},
	month = nov,
	year = {2018},
	keywords = {Paper structure, writing tips, Publication process, Scientific paper writing},
	pages = {1278--1283},
	file = {ScienceDirect Snapshot:files/64/S1319562X18300135.html:text/html;Texto completo:files/65/Meo - 2018 - Anatomy and physiology of a scientific paper.pdf:application/pdf},
}

@book{katz_elements_1985,
	title = {Elements of the {Scientific} {Paper}},
	isbn = {978-0-300-03532-2},
	abstract = {Shared knowledge is indispensable to the practice of science, and the scientific paper--whether published in a journal or collation volume--is the chief means by which scientists communicate ideas and results to their colleagues. Mastering the genre is thus an essential element in every scientist\&\#39;s training.Using a published paper as a guide, Michael J. Katz takes the reader through every step of the writing process, including the use of standard formats (abstract, introduction, materials and methods, results, discussion, acknowledgments, and references), language (style and word usage), and publication (choosing the appropriate journal, the review process, and revising). Other chapters discuss figures (photographs, schematic diagrams, and graphs), writing with a computer, and numbers (algorithms and statistics). Nine appendices provide a handy reference to commonly needed information such as scientific abbreviations, non-technical words, and mathematic formulae. While recognizing that the scientific paper is constrained within a well-defined form, the book also stresses that the genre is narrative prose requiring a lucid, precise, and careful style. The elements of composition--gestation, diction, revision, and rewriting--are discussed in detail.Elements of the Scientific Paper is a useful handbook for young scientists and graduate students beginning their publishing careers, as well as for anyone wishing a review of or introduction to the elements of scientific style.},
	language = {en},
	publisher = {Yale University Press},
	author = {Katz, Michael Jay},
	month = jan,
	year = {1985},
	note = {Google-Books-ID: umYHoNJnygUC},
	keywords = {Technology \& Engineering / Technical Writing},
}

@article{malan_functional_2001,
	title = {Functional requirements capture the intended behavior of the system. {This} behavior may be expressed as services, tasks or functions the system is required to perform. {This} white paper lays out important con- cepts and discusses capturing functional requirements in such a way that they can drive architectural decisions and be used to validate the architecture.},
	language = {en},
	author = {Malan, Ruth and Bredemeyer, Dana and Consulting, Bredemeyer},
	year = {2001},
	file = {Malan et al. - 2001 - Functional requirements capture the intended behav.pdf:files/68/Malan et al. - 2001 - Functional requirements capture the intended behav.pdf:application/pdf},
}

@article{schwaber2020guia,
  title={La gu{\'\i}a definitiva de Scrum: las reglas del juego},
  author={Schwaber, Ken and Sutherland, Jeff},
  year={2020}
}

@misc{Hugging_Face_2016,
    title = {Spaces},
    howpublished ={https://huggingface.co/spaces}, 
    journal={Hugging Face}, 
    publisher={Hugging Face}, 
    year={2016}
} 

@article{chung2000non,
  title={Non-functional requirements},
  author={Chung, Lawrence and Nixon, Brian and Yu, Eric and Mylopoulos, John},
  journal={Software Engineering},
  pages={78},
  year={2000},
  publisher={Kluwer Academic}
}


@misc{CRUE_2005,
    author={{CRUE }},
    title={Directrices_Sosteniblidad},
    url = {https://www.crue.org/wp-content/uploads/2020/02/Directrices_Sosteniblidad_Crue2012.pdf},
    year={2012},
    note={\newline(original work published 2005)},
}

@misc{nestor_maslej_et_al_2024,
	title = {{AI} {Index} {Report} 2024 – {Artificial} {Intelligence} {Index}},
	url = {https://aiindex.stanford.edu/report/},
	urldate = {2024-06-29},
	author = {Nestor Maslej, et al},
	month = apr,
	year = {2024},
	file = {AI Index Report 2024 – Artificial Intelligence Index:C\:\\Users\\Usuario\\Zotero\\storage\\VW4PJBZR\\report.html:text/html},
}

@misc{harrison_chase_langchain,
	title = {{LangChain}: {Chat} with {Your} {Data}},
	shorttitle = {{LangChain}},
	url = {https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/},
	abstract = {Learn how to create a chatbot to interact with your private data using LangChain. Get insights from LangChain creator Harrison Chase.},
	language = {en},
	urldate = {2024-07-01},
	author = {{Harrison Chase}},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\LXHSVNXC\\langchain-chat-with-your-data.html:text/html},
}

@misc{langchain_2022,
	publisher = {LangChain},
	title = {{L}ang{C}hain --- langchain.com},
	howpublished = {https://www.langchain.com},
	year = {2022},
	urldate = {2024-07-01},
}


@misc{pablo_huijse_heise_12_2022,
	title = {12. {Introducción} a la librería {PyTorch} — {Aprendizaje} de {Máquinas}},
	url = {https://phuijse.github.io/MachineLearningBook/contents/neural_networks/torch-tensor.html},
	urldate = {2024-07-01},
	author = {{Pablo Huijse Heise}},
	year = {2022},
	file = {12. Introducción a la librería PyTorch — Aprendizaje de Máquinas:C\:\\Users\\Usuario\\Zotero\\storage\\G3BIGJA9\\torch-tensor.html:text/html},
}

@misc{gonzalez2011python,
  title={Python para todos},
  author={Gonz{\'a}lez Duque, Ra{\'u}l},
  year={2011},
  publisher={Creative Commons Reconocimiento}
}

@misc{ben_lutkevich_what_2023,
	title = {What {Is} {Hugging} {Face}? {\textbar} {Definition} from {TechTarget}},
	shorttitle = {What {Is} {Hugging} {Face}?},
	url = {https://www.techtarget.com/whatis/definition/Hugging-Face},
	abstract = {Discover Hugging Face, an AI company renowned for its Transformers library, offering cutting-edge models for diverse natural language processing tasks.},
	language = {en},
	urldate = {2024-07-01},
	journal = {WhatIs},
	author = {{Ben Lutkevich}},
	month = sep,
	year = {2023},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\XILFSX8C\\Hugging-Face.html:text/html},
}


@misc{herve_jegou_faiss_2017,
	title = {Faiss: {A} library for efficient similarity search},
	shorttitle = {Faiss},
	url = {https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/},
	abstract = {Visit the post for more.},
	language = {en-US},
	urldate = {2024-07-01},
	journal = {Engineering at Meta},
	author = {{Hervé Jegou} and {Matthijs Douze} and {Jeff Johnson}},
	month = mar,
	year = {2017},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\DNA7UMPQ\\faiss-a-library-for-efficient-similarity-search.html:text/html},
}


@misc{jakindah_top_2023,
	title = {Top {P}, {Temperature} and {Other} {Parameters}},
	url = {https://medium.com/@dixnjakindah/top-p-temperature-and-other-parameters-1a53d2f8d7d7},
	abstract = {Large Language Models(LLMs) are essential tools in natural language processing (NLP) and have been used in a variety of applications, such…},
	language = {en},
	urldate = {2024-07-03},
	journal = {Medium},
	author = {Jakindah, Dixn},
	month = may,
	year = {2023},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\YVS5K8SI\\top-p-temperature-and-other-parameters-1a53d2f8d7d7.html:text/html},
}

@INPROCEEDINGS{Hayat_2019,
  author={Hayat, Faisal and Rehman, Ammar Ur and Arif, Khawaja Sarmad and Wahab, Kanwal and Abbas, Muhammad},
  booktitle={2019 20th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)}, 
  title={The Influence of Agile Methodology (Scrum) on Software Project Management}, 
  year={2019},
  volume={},
  number={},
  pages={145-149},
  keywords={Software;Scrum (Software development);Project management;Companies;Bibliographies;Software engineering;Agile Methodology;Scrum;Project Knowledge Areas;Software Project Management},
  doi={10.1109/SNPD.2019.8935813}}


@misc{ken_schwaber_scrum_2020,
	title = {Scrum {Guide} {\textbar} {Scrum} {Guides}},
	url = {https://scrumguides.org/scrum-guide.html},
	urldate = {2024-07-03},
	author = {{Ken Schwaber} and {Jeff Sutherland}},
	year = {2020},
	file = {Scrum Guide | Scrum Guides:C\:\\Users\\Usuario\\Zotero\\storage\\XDM6H4FN\\scrum-guide.html:text/html},
}
