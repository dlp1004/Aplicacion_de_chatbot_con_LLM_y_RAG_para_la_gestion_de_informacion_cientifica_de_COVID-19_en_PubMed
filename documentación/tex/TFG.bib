
@article{kim_chatgpt_2023,
	title = {{ChatGPT} and large language model ({LLM}) chatbots: {The} current state of acceptability and a proposal for guidelines on utilization in academic medicine},
	volume = {19},
	issn = {14775131},
	shorttitle = {{ChatGPT} and large language model ({LLM}) chatbots},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1477513123002243},
	doi = {10.1016/j.jpurol.2023.05.018},
	language = {en},
	number = {5},
	urldate = {2024-05-02},
	journal = {Journal of Pediatric Urology},
	author = {Kim, Jin K. and Chua, Michael and Rickard, Mandy and Lorenzo, Armando},
	month = oct,
	year = {2023},
	pages = {598--604},
}

@article{thirunavukarasu_large_2023,
	title = {Large language models in medicine},
	volume = {29},
	issn = {1078-8956, 1546-170X},
	url = {https://www.nature.com/articles/s41591-023-02448-8},
	doi = {10.1038/s41591-023-02448-8},
	language = {en},
	number = {8},
	urldate = {2024-05-02},
	journal = {Nature Medicine},
	author = {Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei},
	month = aug,
	year = {2023},
	pages = {1930--1940},
}

@misc{noauthor_7_2024,
	title = {7 {LLM} use cases and applications in 2024},
	url = {https://www.assemblyai.com/blog/llm-use-cases/},
	abstract = {Learn about the top LLM use cases and applications in 2024 for streamlining business operations, automating mundane tasks, and tackling challenges.},
	language = {en},
	urldate = {2024-05-03},
	journal = {News, Tutorials, AI Research},
	month = mar,
	year = {2024},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\87KBVY7V\\llm-use-cases.html:text/html},
}

@misc{noauthor_what_nodate,
	title = {What is an {AI} {Prompt}? {\textbar} {Definition} from {TechTarget}},
	shorttitle = {What is an {AI} {Prompt}?},
	url = {https://www.techtarget.com/searchenterpriseai/definition/AI-prompt},
	abstract = {Gain insights into the importance and functionality of AI prompts. Delve into their pros and cons and discover the art of crafting efficient prompts.},
	language = {en},
	urldate = {2024-05-03},
	journal = {Enterprise AI},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\RV8TQFKL\\AI-prompt.html:text/html},
}

@article{heston_prompt_2023,
	title = {Prompt {Engineering} in {Medical} {Education}},
	volume = {2},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2813-141X},
	url = {https://www.mdpi.com/2813-141X/2/3/19},
	doi = {10.3390/ime2030019},
	abstract = {Artificial intelligence-powered generative language models (GLMs), such as ChatGPT, Perplexity AI, and Google Bard, have the potential to provide personalized learning, unlimited practice opportunities, and interactive engagement 24/7, with immediate feedback. However, to fully utilize GLMs, properly formulated instructions are essential. Prompt engineering is a systematic approach to effectively communicating with GLMs to achieve the desired results. Well-crafted prompts yield good responses from the GLM, while poorly constructed prompts will lead to unsatisfactory responses. Besides the challenges of prompt engineering, significant concerns are associated with using GLMs in medical education, including ensuring accuracy, mitigating bias, maintaining privacy, and avoiding excessive reliance on technology. Future directions involve developing more sophisticated prompt engineering techniques, integrating GLMs with other technologies, creating personalized learning pathways, and researching the effectiveness of GLMs in medical education.},
	language = {en},
	number = {3},
	urldate = {2024-05-03},
	journal = {International Medical Education},
	author = {Heston, Thomas and Khun, Charya},
	month = aug,
	year = {2023},
	pages = {198--205},
	file = {Texto completo:C\:\\Users\\Usuario\\Zotero\\storage\\UHEDFNUQ\\Heston y Khun - 2023 - Prompt Engineering in Medical Education.pdf:application/pdf},
}

@misc{noauthor_2_nodate,
	title = {(2) 9 {Frameworks} to master {ChatGPT} {Prompt} {Engineering}. {\textbar} {LinkedIn}},
	url = {https://www.linkedin.com/pulse/9-frameworks-master-chatgpt-prompt-engineering-edi-hezri-hairi/},
	urldate = {2024-05-04},
	file = {(2) 9 Frameworks to master ChatGPT Prompt Engineering. | LinkedIn:C\:\\Users\\Usuario\\Zotero\\storage\\THX5B42C\\9-frameworks-master-chatgpt-prompt-engineering-edi-hezri-hairi.html:text/html},
}

@misc{noauthor_what_nodate-1,
	title = {What is an {LLM}? {A} {Guide} on {Large} {Language} {Models} and {How} {They} {Work}},
	shorttitle = {What is an {LLM}?},
	url = {https://www.datacamp.com/blog/what-is-an-llm-a-guide-on-large-language-models},
	abstract = {Read this article to discover the basics of large language models, the key technology that is powering the current AI revolution},
	language = {en},
	urldate = {2024-05-05},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\DLTDATKW\\what-is-an-llm-a-guide-on-large-language-models.html:text/html},
}

@misc{noauthor_106_nodate,
	title = {10.6. {The} {Encoder}–{Decoder} {Architecture} — {Dive} into {Deep} {Learning} 1.0.3 documentation},
	url = {https://d2l.ai/chapter_recurrent-modern/encoder-decoder.html},
	urldate = {2024-05-05},
	file = {10.6. The Encoder–Decoder Architecture — Dive into Deep Learning 1.0.3 documentation:C\:\\Users\\Usuario\\Zotero\\storage\\UKEYW2J9\\encoder-decoder.html:text/html},
}

@misc{kostadinov_understanding_2019,
	title = {Understanding {Encoder}-{Decoder} {Sequence} to {Sequence} {Model}},
	url = {https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346},
	abstract = {In this article, I will try to give a short and concise explanation of the sequence to sequence model which have recently achieved…},
	language = {en},
	urldate = {2024-05-05},
	journal = {Medium},
	author = {Kostadinov, Simeon},
	month = nov,
	year = {2019},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\WLCHVJXL\\understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346.html:text/html},
}

@misc{kumar_demystifying_2024,
	title = {Demystifying {Encoder} {Decoder} {Architecture} \& {Neural} {Network}},
	url = {https://vitalflux.com/encoder-decoder-architecture-neural-network/},
	abstract = {Encoder decoder architecture, Encoder Architecture, Decoder Architecture, BERT, GPT, T5, BART, Examples, NLP, Transformers, Machine Learning},
	language = {en-US},
	urldate = {2024-05-05},
	journal = {Analytics Yogi},
	author = {Kumar, Ajitesh},
	month = jan,
	year = {2024},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\LT2MLNQ6\\encoder-decoder-architecture-neural-network.html:text/html},
}

@misc{ahmadsabry_perfect_2023,
	title = {A {Perfect} guide to {Understand} {Encoder} {Decoders} in {Depth} with {Visuals}},
	url = {https://medium.com/@ahmadsabry678/a-perfect-guide-to-understand-encoder-decoders-in-depth-with-visuals-30805c23659b},
	abstract = {Introduction},
	language = {en},
	urldate = {2024-05-05},
	journal = {Medium},
	author = {Ahmadsabry},
	month = jun,
	year = {2023},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\QRVL4DCC\\a-perfect-guide-to-understand-encoder-decoders-in-depth-with-visuals-30805c23659b.html:text/html},
}

@article{peng_instruction_2023,
	title = {Instruction {Tuning} with {GPT}-4},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2304.03277},
	doi = {10.48550/ARXIV.2304.03277},
	abstract = {Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.},
	urldate = {2024-05-05},
	author = {Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
	year = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
	annote = {Other
8 pages. Work in progress. Project page: https://instruction-tuning-with-gpt-4.github.io},
}

@misc{noauthor_rag_nodate,
	title = {{RAG} - {Retrieval} {Augmented} {Generation}: {The} key that unlocks the door to precision language models {\textbar} datos.gob.es},
	url = {https://datos.gob.es/en/blog/rag-retrieval-augmented-generation-key-unlocks-door-precision-language-models},
	urldate = {2024-05-07},
	file = {RAG - Retrieval Augmented Generation\: The key that unlocks the door to precision language models | datos.gob.es:C\:\\Users\\Usuario\\Zotero\\storage\\ULKCP58I\\rag-retrieval-augmented-generation-key-unlocks-door-precision-language-models.html:text/html},
}

@misc{noauthor_what_nodate-2,
	title = {What is {Retrieval} {Augmented} {Generation} ({RAG})? {A} {Guide} to the {Basics}},
	shorttitle = {What is {Retrieval} {Augmented} {Generation} ({RAG})?},
	url = {https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag},
	abstract = {Explore Retrieval Augmented Generation (RAG) RAG: Integrating LLMs with data search for nuanced AI responses. Understand its applications and impact.},
	language = {en},
	urldate = {2024-05-07},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\UT2RLBUI\\what-is-retrieval-augmented-generation-rag.html:text/html},
}

@misc{noauthor_about_nodate,
	title = {About},
	url = {https://pubmed.ncbi.nlm.nih.gov/about/},
	abstract = {PubMed about page.},
	language = {en},
	urldate = {2024-05-07},
	journal = {PubMed},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\RB4Q7SRC\\about.html:text/html},
}

@article{habas_resolution_2020,
	title = {Resolution of coronavirus disease 2019 ({COVID}-19)},
	volume = {18},
	issn = {1478-7210},
	url = {https://doi.org/10.1080/14787210.2020.1797487},
	doi = {10.1080/14787210.2020.1797487},
	abstract = {Coronavirus disease 2019 (COVID-19) was first detected in China in December, 2019, and declared as a pandemic by the World Health Organization (WHO) on March 11, 2020. The current management of COVID-19 is based generally on supportive therapy and treatment to prevent respiratory failure. The effective option of antiviral therapy and vaccination are currently under evaluation and development. A literature search was performed using PubMed between December 1, 2019–June 23, 2020. This review highlights the current state of knowledge on the viral replication and pathogenicity, diagnostic and therapeutic strategies, and management of COVID-19. This review will be of interest to scientists and clinicians and make a significant contribution toward development of vaccines and targeted therapies to contain the pandemic. The exit strategy for a path back to normal life is required, which should involve a multi-prong effort toward development of new treatment and a successful vaccine to protect public health worldwide and prevent future COVID-19 outbreaks. Therefore, the bench to bedside translational research as well as reverse translational works focusing bedside to bench is very important and would provide the foundation for the development of targeted drugs and vaccines for COVID-19 infections.},
	number = {12},
	urldate = {2024-05-07},
	journal = {Expert Review of Anti-infective Therapy},
	author = {Habas, Khaled and Nganwuchu, Chioma and Shahzad, Fanila and Gopalan, Rajendran and Haque, Mainul and Rahman, Sayeeda and Majumder, Anwarul Azim and Nasim, Talat},
	month = dec,
	year = {2020},
	pmid = {32749914},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/14787210.2020.1797487},
	keywords = {COVID-19, diagnosis, SARS-CoV-2, severe acute respiratory syndrome coronavirus-2, treatment},
	pages = {1201--1211},
}

@article{clusmann_future_2023,
	title = {The future landscape of large language models in medicine},
	volume = {3},
	copyright = {2023 The Author(s)},
	issn = {2730-664X},
	url = {https://www.nature.com/articles/s43856-023-00370-1},
	doi = {10.1038/s43856-023-00370-1},
	abstract = {Large language models (LLMs) are artificial intelligence (AI) tools specifically trained to process and generate text. LLMs attracted substantial public attention after OpenAI’s ChatGPT was made publicly available in November 2022. LLMs can often answer questions, summarize, paraphrase and translate text on a level that is nearly indistinguishable from human capabilities. The possibility to actively interact with models like ChatGPT makes LLMs attractive tools in various fields, including medicine. While these models have the potential to democratize medical knowledge and facilitate access to healthcare, they could equally distribute misinformation and exacerbate scientific misconduct due to a lack of accountability and transparency. In this article, we provide a systematic and comprehensive overview of the potentials and limitations of LLMs in clinical practice, medical research and medical education.},
	language = {en},
	number = {1},
	urldate = {2024-05-08},
	journal = {Communications Medicine},
	author = {Clusmann, Jan and Kolbinger, Fiona R. and Muti, Hannah Sophie and Carrero, Zunamys I. and Eckardt, Jan-Niklas and Laleh, Narmin Ghaffari and Löffler, Chiara Maria Lavinia and Schwarzkopf, Sophie-Caroline and Unger, Michaela and Veldhuizen, Gregory P. and Wagner, Sophia J. and Kather, Jakob Nikolas},
	month = oct,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Health services, Medical research, Public health},
	pages = {1--8},
	file = {Full Text PDF:C\:\\Users\\Usuario\\Zotero\\storage\\EL7D2H5G\\Clusmann et al. - 2023 - The future landscape of large language models in m.pdf:application/pdf},
}

@misc{nucci_large_2023,
	title = {Large {Language} {Models} in {Healthcare}: {Use} {Cases} and {Benefits}},
	shorttitle = {Large {Language} {Models} in {Healthcare}},
	url = {https://aisera.com/blog/large-language-models-healthcare/},
	abstract = {Large language models revolutionize healthcare with innovative diagnostics, clinical documentation, and patient care solutions. Learn more!},
	language = {en-US},
	urldate = {2024-05-08},
	journal = {Aisera: Best Generative AI Platform For Enterprise},
	author = {Nucci, Antonio},
	month = dec,
	year = {2023},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\TFP98CJN\\large-language-models-healthcare.html:text/html},
}

@misc{noauthor_best_nodate,
	title = {The best large language models ({LLMs}) in 2024},
	url = {https://zapier.com/blog/best-llm/#gemma},
	abstract = {There are dozens of major LLMs, and hundreds that are arguably significant for some reason or other. These are 14 of the best LLMs available now.},
	language = {en},
	urldate = {2024-05-08},
	file = {Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\WSYIFKRD\\best-llm.html:text/html},
}

@misc{noauthor_meta_nodate,
	title = {Meta {Llama} 3},
	url = {https://llama.meta.com/llama3/},
	abstract = {Build the future of AI with Meta Llama 3. Now available with both 8B and 70B pretrained and instruction-tuned versions to support a wide range of applications.},
	language = {en},
	urldate = {2024-05-08},
	journal = {Meta Llama},
}

@misc{wu_how_2024,
	title = {How faithful are {RAG} models? {Quantifying} the tug-of-war between {RAG} and {LLMs}' internal prior},
	shorttitle = {How faithful are {RAG} models?},
	url = {http://arxiv.org/abs/2404.10198},
	doi = {10.48550/arXiv.2404.10198},
	abstract = {Retrieval augmented generation (RAG) is often used to fix hallucinations and provide up-to-date knowledge for large language models (LLMs). However, in cases when the LLM alone incorrectly answers a question, does providing the correct retrieved content always fix the error? Conversely, in cases where the retrieved content is incorrect, does the LLM know to ignore the wrong information, or does it recapitulate the error? To answer these questions, we systematically analyze the tug-of-war between a LLM's internal knowledge (i.e. its prior) and the retrieved information in settings when they disagree. We test GPT-4 and other LLMs on question-answering abilities across datasets with and without reference documents. As expected, providing the correct retrieved information fixes most model mistakes (94\% accuracy). However, when the reference document is perturbed with increasing levels of wrong values, the LLM is more likely to recite the incorrect, modified information when its internal prior is weaker but is more resistant when its prior is stronger. Similarly, we also find that the more the modified information deviates from the model's prior, the less likely the model is to prefer it. These results highlight an underlying tension between a model's prior knowledge and the information presented in reference documents.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Wu, Kevin and Wu, Eric and Zou, James},
	month = apr,
	year = {2024},
	note = {arXiv:2404.10198 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Usuario\\Zotero\\storage\\7UFEAXX6\\Wu et al. - 2024 - How faithful are RAG models Quantifying the tug-o.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Usuario\\Zotero\\storage\\PDF8UFPG\\2404.html:text/html},
}

@misc{noauthor_what_nodate-3,
	title = {What is retrieval-augmented generation ({RAG})? - {IBM} {Research}},
	url = {https://research.ibm.com/blog/retrieval-augmented-generation-RAG},
	urldate = {2024-05-08},
}

@misc{noauthor_what_nodate-4,
	title = {What {Is} {Retrieval}-{Augmented} {Generation} ({RAG})?},
	url = {https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/},
	urldate = {2024-05-08},
	file = {What Is Retrieval-Augmented Generation (RAG)?:C\:\\Users\\Usuario\\Zotero\\storage\\PDB59TBF\\retrieval-augmented-generation-rag.html:text/html},
}

@article{gomez_aplicacion_2020,
	title = {Aplicación de las {Metodologías} Ágiles al proceso de enseñanza-aprendizaje universitario},
	copyright = {Drets d'autor (c) 2020 Revista d'Innovació Docent Universitària},
	issn = {2013-2298},
	url = {https://revistes.ub.edu/index.php/RIDU/article/view/RIDU2020.12.7},
	doi = {10.1344/RIDU2020.12.7},
	abstract = {Aquesta proposta d'innovació docent té com a objectiu el de dinamitzar i apropar al món empresarial l'ensenyament  en l'assignatura de sis crèdits Gestió i organització d'Empreses II, cursada en el quart semestre dels diferents graus que s'ofereixen a la Facultad de Ciencias Económicas de la Universidad San Pablo-CEU, de forma que l'alumne participi de forma més activa del que fins ara venia fent i s'impliqui en la solució dels problemes d'empreses reals plantejats a l'aula. Amb aquesta finalitat s'utilizaran metodologies àgils i innovacions tecnològiques, que permetin aplicar de forma pràctica l'aprenentatge basat en projectes (APB).Per a la seva implementació es proposa la resolució d'un cas de producció per tallers, mitjançant la formació d'equips àgils, utilitzant Scrum i Kanban, fent servir per al darrer, l'aplicació Trello. L'enfoc iteratiu i incremental d'aquesta metodologia permetrà que l'alumne conegui de forma senzilla i pràctica conceptes que creixen en quant a la seva complexitat, segons es va evolucionant en la resolució del mateix.Per a l'assignatura en qüestió, el desenvolupament de la solució es farà en cinc iteracions o sprints, afegint a cada iteració nous resultats d'aprenentatge, que permetin aconseguir les competències marcades en la matèria.},
	language = {es},
	urldate = {2024-05-30},
	journal = {Revista d'Innovació Docent Universitària},
	author = {Gómez, Sonia Martín},
	month = jan,
	year = {2020},
	keywords = {Trello},
	pages = {62--73},
	file = {Full Text PDF:C\:\\Users\\Usuario\\Zotero\\storage\\5UE36PQB\\Gómez - 2020 - Aplicación de las Metodologías Ágiles al proceso d.pdf:application/pdf},
}
